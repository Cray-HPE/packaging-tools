#!/usr/bin/env python3

# Copyright 2020 Hewlett Packard Enterprise Development LP

import asyncio
from email.utils import parsedate_tz, mktime_tz
import hashlib
import io
import logging
import os
import os.path
import sys
from time import time
from urllib.parse import urljoin, urlparse

import aiohttp
import yaml

LOGGER = logging.getLogger('helm-sync')

def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('-n', metavar='INT', type=int, default=1, help="Number of charts to download at a time")
    parser.add_argument('--scoped', action='store_true', default=False, help="Download charts to repository-specific directories")
    parser.add_argument('--disable-cache', dest='cache_enabled', action='store_false', default=True, help="Always download, do not use cache")
    parser.add_argument('--dry-run', action='store_true', default=False, help="Verify charts exist in repository but do not download")
    parser.add_argument('-v', '--verbose', action='count', default=0, help="Verbosity")
    parser.add_argument('index', metavar='INDEX', type=argparse.FileType('r'), help="Chart index")
    parser.add_argument('destdir', metavar='DIR', nargs='?', default=os.getcwd(), help="Destination directory")
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.WARNING,
        format="%(asctime)s\t%(levelname)s\t%(message)s",
    )
    log_levels = [logging.WARNING, logging.INFO, logging.DEBUG]
    LOGGER.setLevel(log_levels[min(len(log_levels)-1, args.verbose)])

    index = yaml.safe_load(args.index)

    asyncio.run(sync(index, args.destdir, args.n, args.scoped, args.cache_enabled, args.dry_run))


async def sync(index, destdir, n=1, scoped=False, cache_enabled=True, dry_run=False):
    sem = asyncio.Semaphore(n)
    async with aiohttp.ClientSession() as session:
        for url, repo in index.items():
            if not repo['charts']:
                LOGGER.warning(f'skipping {repo_name}, no charts specified')
                continue
            repo['url'] = url
            if scoped:
                _url = urlparse(url)
                _destdir = os.path.join(destdir, _url.netloc, _url.path.lstrip(os.path.sep))
            else:
                _destdir = destdir
            await sync_repo(sem, session, repo, _destdir, cache_enabled, dry_run)
        # wait for all tasks to finish
        tasks = asyncio.all_tasks()
        tasks.remove(asyncio.current_task())
        if tasks:
            await asyncio.gather(*tasks)


async def sync_repo(sem, session, repo, destdir, cache_enabled=True, dry_run=False):
    LOGGER.info(f'syncing {repo["url"]}')

    # Get repository's index
    index_url = urljoin(repo['url'], 'index.yaml')
    LOGGER.debug(f'getting index: {index_url}')
    async with sem, session.get(index_url, raise_for_status=True) as response:
        repo['index'] = yaml.safe_load(io.StringIO(await response.text()))
        LOGGER.info(f'[{response.status} {response.reason}] {index_url}')

    # Verify index
    if 'entries' not in repo['index']:
        LOGGER.error(f'invalid index: {index_url}')
        asyncio.get_running_loop().stop()

    charts = []
    any_errors = False
    for chart_name, chart_versions in repo['charts'].items():

        # Verify chart exists in index
        if chart_name not in repo['index']['entries']:
            LOGGER.error(f'index missing {chart_name} chart: {index_url}')
            any_errors = True
            continue

        # Filter index to get available versions of chart
        available_versions = {i['version']: i for i in repo['index']['entries'][chart_name] if i['version'] in chart_versions}

        for version in chart_versions:
            try:
                metadata = available_versions[version]
            except KeyError:
                LOGGER.error(f'index missing {chart_name} version {version}: {index_url}')
                any_errors = True
                continue

            path = os.path.join(destdir, f'{chart_name}-{version}.tgz')

            # Verify existing path
            if cache_enabled and os.path.isfile(path) and verify_digest(path, metadata['digest']):
                LOGGER.info(f'[cached] {path}')
                continue

            charts.append(([urljoin(repo['url'], url) for url in metadata['urls']], path))

    if any_errors:
        asyncio.get_running_loop().stop()

    for urls, path in charts:
        # Download chart
        await sem.acquire()
        asyncio.create_task(download_chart(sem, session, urls, path, dry_run))


def verify_digest(path, digest, bufsize=(1 << 16)):
    m = hashlib.sha256()
    with open(path, 'rb') as f:
        while True:
            chunk = f.read(bufsize)
            if not chunk:
                break
            m.update(chunk)
    return m.hexdigest() == digest


async def download_chart(sem, session, urls, path, dry_run=False):
    fetch = session.head if dry_run else session.get
    try:
        LOGGER.debug(f'downloading {path}')
        # take first URL we can find
        for url in urls:
            async with session.get(url) as response:
                # check response
                if response.status >= 400:
                    LOGGER.warning(f'[{response.status} {response.reason}] {url}')
                    continue
                LOGGER.info(f'[{response.status} {response.reason}] {url}')
                if not dry_run:
                    # create directory
                    if not os.path.isdir(os.path.dirname(path)):
                        os.makedirs(os.path.dirname(path))
                    # write chart file
                    with open(path, 'wb') as f:
                        while True:
                            chunk = await response.content.read(8192)
                            if not chunk:
                                break
                            f.write(chunk)
                    # set file last modified time
                    modified = response.headers.get('last-modified')
                    if modified:
                        os.utime(path, (time(), mktime_tz(parsedate_tz(modified))))
                print(path, file=sys.stdout, flush=True)
                return
    finally:
        sem.release()
    # no URLs were valid!
    LOGGER.error(f'failed to download chart: {path}')
    asyncio.get_running_loop().stop()


if __name__ == "__main__":
    main()
