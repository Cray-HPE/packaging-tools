#!/usr/bin/env python3

# Copyright 2020 Hewlett Packard Enterprise Development LP

import asyncio
from email.utils import parsedate_tz, mktime_tz
import hashlib
import logging
import os
import os.path
import sys
from time import time
from urllib.parse import urljoin, urlparse

import aiohttp
import repomd
import yaml

LOGGER = logging.getLogger('rpm-sync')


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--destdir', metavar='DIR', help="Destination directory")
    parser.add_argument('-n', metavar='INT', type=int, default=1, help="Number of assets to download at a time")
    parser.add_argument('--scoped', action='store_true', default=False, help="Download assets to repository-specific directories")
    parser.add_argument('--disable-cache', dest='cache_enabled', action='store_false', default=True, help="Always download, do not use cache")
    parser.add_argument('--dry-run', action='store_true', default=False, help="Verify charts exist in repository but do not download")
    parser.add_argument('-v', '--verbose', action='count', default=0, help="Verbosity")
    parser.add_argument('index', metavar='INDEX', type=argparse.FileType('r'), help="RPM index")
    args = parser.parse_args()

    if not args.destdir:
        args.destdir = os.path.dirname(args.index.name)

    logging.basicConfig(
        level=logging.WARNING,
        format="%(asctime)s\t%(levelname)s\t%(message)s",
    )
    log_levels = [logging.WARNING, logging.INFO, logging.DEBUG]
    LOGGER.setLevel(log_levels[min(len(log_levels)-1, args.verbose)])

    index = yaml.safe_load(args.index)
    if not index:
        return

    assets = [] # (location, digest, repourl, destdir)

    for url, spec in index.items():
        # Fix-up base URL, ensure trailing slash
        url = url.rstrip(os.path.sep) + os.path.sep

        if not spec.get('rpms'):
            LOGGER.warning(f'Skipping {url}, no RPMs specified')
            continue
        LOGGER.info(f'Reading {url}')

        # Clean up RPMs
        rpms = list(map(lambda x: x[:-4] if x.endswith('.rpm') else x, spec['rpms']))

        # Load repo metadata
        repo = repomd.load(url)

        # Read package metadata
        pkgs = {}
        for el in repo._metadata.findall('common:package', namespaces=repomd._ns):
            p = repomd.Package(el)
            if p.nevra not in rpms:
                continue

            # Source URL
            srcurl = urljoin(repo.baseurl, p.location)

            # Destination path
            if args.scoped:
                _url = urlparse(srcurl)
                destpath = os.path.join(args.destdir, _url.netloc, _url.path.lstrip(os.path.sep))
            else:
                destpath = os.path.join(args.destdir, spec.get('dir', '{arch}').format(arch=p.arch), os.path.basename(p.location))

            # SHA256 digest
            digest = p._element.findtext('common:checksum[@type="sha256"]', namespaces=repomd._ns)

            # Save source, destination, and digest
            pkgs[p.nevra] = (srcurl, destpath, digest)

        # Add each RPM to asset list
        for name in rpms:
            # Ensure no RPMs were missed
            try:
                assets.append(pkgs[name])
            except KeyError:
                LOGGER.error(f'{name} not found in repo at {url}')
                sys.exit(1)

    asyncio.run(sync(assets, args.n, args.cache_enabled, args.dry_run))


async def sync(assets, n=1, cache_enabled=True, dry_run=False):
    sem = asyncio.Semaphore(n)
    async with aiohttp.ClientSession() as session:
        for url, path, digest in assets:
            # Verify existing path
            if cache_enabled and os.path.isfile(path) and digest and verify_digest(path, digest):
                LOGGER.info(f'[cached] {path}')
                continue

            await sem.acquire()
            asyncio.create_task(download_asset(sem, session, [url], path, dry_run))

        # wait for all tasks to finish
        tasks = asyncio.all_tasks()
        tasks.remove(asyncio.current_task())
        if tasks:
            await asyncio.gather(*tasks)


def verify_digest(path, digest, bufsize=(1 << 16)):
    m = hashlib.sha256()
    with open(path, 'rb') as f:
        while True:
            chunk = f.read(bufsize)
            if not chunk:
                break
            m.update(chunk)
    return m.hexdigest() == digest


async def download_asset(sem, session, urls, path, dry_run=False):
    """Download asset from one of the given `urls` to the specified `path`."""
    fetch = session.head if dry_run else session.get
    try:
        LOGGER.debug(f'downloading {path}')
        # take first URL we can find
        for url in urls:
            async with fetch(url) as response:
                # check response
                if response.status >= 400:
                    LOGGER.warning(f'[{response.status} {response.reason}] {url}')
                    continue
                LOGGER.info(f'[{response.status} {response.reason}] {url}')
                if not dry_run:
                    # create directory
                    if not os.path.isdir(os.path.dirname(path)):
                        os.makedirs(os.path.dirname(path))
                    # write asset file
                    with open(path, 'wb') as f:
                        while True:
                            chunk = await response.content.read(8192)
                            if not chunk:
                                break
                            f.write(chunk)
                    # set file last modified time
                    modified = response.headers.get('last-modified')
                    if modified:
                        os.utime(path, (time(), mktime_tz(parsedate_tz(modified))))
                print(path, file=sys.stdout, flush=True)
                return
    finally:
        sem.release()
    # no URLs were valid!
    LOGGER.error(f'failed to download asset: {path}')
    asyncio.get_running_loop().stop()


if __name__ == "__main__":
    main()

